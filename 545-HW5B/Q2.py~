import numpy as np
import math

def sigmoid(l):
    return [1/(1.0+ math.exp(-a)) for a in l]

def softmax(l):
    result = [math.exp(-a) for a in l]
    return np.array(result)/np.sum(result)

#def part2():

# forward and backward propagation
def forbac_pro(x, label, w1, w2):
	# x ~ (1, 401), w1 ~ (401, 16), w2 ~ (17, 10)
    S_in =  np.dot(x, w1)                         # S_in is the input of the hidden layer
    S_out = sigmoid(S_in)
    S_out.insert(0,1)
    S_out = np.array(S_out)                        # S_out ~ (1, 17)

    M_in = np.dot(S_out, w2)                       # M_in: input of the softmax layer
    M_out = softmax(M_in)                          # M_out ~(1,10)

    # gradient of log likelihood w.r.t w2
    gd_w2 = np.tile(S_out, (10,1)).T * (-M_out)
    gd_w2[:,label] = gd_w2[:,label] + S_out


    # gradient of log likelihood w.r.t S_out
    gd_sout = np.sum( w2 * M_out,axis = 1) - w2[:, label]

    # gradient of log likelihood w.r.t S
    gd_s = S_out * ( 1 - S_out) * gd_sout

    # gradient of log likelihood w.r.t w1
    #gd_w1 = np.tile(x, (10,1)).T * gd_s
    gd_w1 = np.zeros((401,16))
    for i in range(16):
        gd_w1[:,i] = x * gd_s[i+1]

    return gd_w1, gd_w2

def train(data):
	# randomly init w to be uniformly distributed in [-0.12,0.12]
    w1 = np.random.rand(401, 16) * 0.24 - 0.12
    w2 = np.random.rand(17, 10) * 0.24 - 0.12

    for i in range(600):
        grad1, grad2 = np.zeros((401, 16)), np.zeros((17, 10))
    	for dp in data:
    		g1, g2 = forbac_pro(dp[:-1], dp[-1], w1, w2)                   # compute gradient of w1 and w2 
    		grad1 += g1 + 0.5 * lmd * w1
    		grad2 += g2 + 0.5 * lmd * w2
    	w1 -= grad1 * LR
    	w2 -= grad2 * LR

def part1():
    train_data = np.genfromtxt('train.csv', delimiter=',')
    #label = train_data[:,-1]
    #train_data = train_data[:,:-1]
    train_data = np.hstack((np.ones((len(train_data),1)),train_data))

    train(train_data)

lmd = 0.001               # regularization term
LR = .0005                  # learning rate

part1()
